% vim: set spelllang=fr,en foldmethod=marker:
\section{Résolution des jeux}
\label{tj:sec:algo}

Cette section s'intéresse au problème posé par la résolution de ces jeux.
Nous montrerons tout d'abord que la résolution est un problème indécidable lorsque la condition de victoire est une formule de gain arbitraire.
Nous nous concentrerons ensuite sur le segment positif de ces formules, et donnerons des conditions suffisantes pour pouvoir résoudre le problème.

\newcommand\jo{joueur~0\xspace}
\newcommand\ji{joueur~1\xspace}
Afin de clarifier la présentation, nous considérons dans cette section deux joueurs, simplement désignés par les termes \jo et \ji.
Le \jo est celui qui cherche à atteindre l'objectif décrit par la formule de gain, et représente donc le nœud compromis.
Conformément à la convention adoptée dans la section précédente, ses états seront représentés sous forme de rectangles.
Le \ji quant à lui représente l'« équipe » (l'ensemble) des autres nœuds, et ses états seront de forme circulaire.

\subsection{Indécidabilité du cas général}

Nous allons démontrer ici que le problème de victoire pour les jeux étudiés ne présente pas de solution algorithmique si la condition de victoire ne peut pas être spécifiée par une formule de gain.
Le théorème établi est le suivant:
\begin{theorem}
Le problème de victoire, ainsi que le problème de crédit initial, sont indécidables pour des objectifs définis par une formule de gain comportant quatre composantes relatives à l'énergie et une composante relative au gain.
\end{theorem}

La preuve de ce théorème consiste à encoder, à l'aide d'un jeu du type étudié, le problème de terminaison à l'aide d'une machine à deux compteurs: le problème équivalent ainsi obtenu est connu comme étant indécidable\cite{minsky67}.
Chaque compteur est représenté par deux composantes d'énergie (une pour chacun des deux joueurs), qui contiennent chacune une copie de la valeur du compteur en question.
Pour l'instruction $0$-test (présentée ci-dessous), le \jo peut prétendre que son compteur est nul, ou bien qu'il ne l'est pas; le second joueur peut vérifier si cette affirmation est valide ou non.
Si la machine atteint son état d'arrêt, une composante de gain est incrémentée, et tous les autres composantes sont réinitialisées à $0$.
De cette manière, si la machine s'arrête, le \jo a une stratégie gagnante lui permettant d'obtenir un gain strictement positif.
Autrement, le gain demeure nul.

On définit plus précisément une machine à deux compteurs comme une liste d'instructions étiquetées, qui peuvent être:
\begin{itemize}
    \item \textbf{increment:} incrémenter le compteur $c$ (respectivement $d$) et aller à l'étiquette~$\ell$
    \item \textbf{0-test:} si $c=0$ (respectivement $d=0$) alors aller à l'étiquette~$\ell_1$, sinon décrémenter $c$ (respectivement $d$) et aller à l'étiquette~$\ell_2$
    \item \textbf{halting:} \emph{arrêt}
\end{itemize}
Au départ, les deux compteurs sont initialisés à $0$.

À présent, étant donné une machine $\machine$, nous construisons une arène $\G_\machine$ basée sur des modules « élémentaires » pour chaque type d'instructions (celui du compteur $c$ sera symétrique à celui du compteur $d$).
L'arène comporte donc 5 composantes $c$,$c'$,$d$,$d'$,$p$, où seule $p$ concerne le gain, les autres étant attachées à des valeurs énergétiques.
Les composantes $c$ et $c'$ (respectivement $d$ et $d'$) sont supposées toujours conserver la même valeur, excepté pour l'instruction $0$-test.
Au lancement du jeu toutes les composantes ont pour valeur\footnote{Pour obtenir une simulation plus réaliste, il est possible d'assigner aux composantes la valeur initiale $1$, et de considérer $0$ comme valeur de défaite. Nous choisissons ici d'autoriser un niveau d'énergie nul et considérons le jeu comme « perdu » pour des valeurs strictement négative, et ce afin de rendre plus clair le lien entre les valeurs des compteurs et les niveaux d'énergie.} $0$.

Le but du \jo est d'obtenir un gain strictement positif, tout en simulant fidèlement la machine.
Il faut entendre par cette expression que les valeurs des composantes $c$ et $d$ doivent demeurer positives.
Les composantes $c'$ et $d'$ permettent de s'assurer que le \ji ne pourra prétendre à tord que le \jo a triché.
Si le cas devait se produire, les valeurs de ces composantes seraient strictement négatives.
À présent la condition de victoire s'exprime comme la satisfaction de la formule de gain suivante:
\[(c\geq0 \wedge d\geq0 \wedge p>0) \vee (c'<0 \wedge c\geq0) \vee (d'<0 \wedge d\geq0)\]

Chaque instruction étiquetée a un état dans l'arène appartenant au \jo, avec la même étiquette (d'autres états seront rajoutés par la suite).
Le cas de l'incrément est trivial: si l'instruction $\ell$ est « incrémenter $c$ puis aller à l'étiquette~$\ell'$ », alors il y a une arête partant de $\ell$ et joignant $\ell'$ avec les poids $(1,1,0,0,0)$.
Il n'y a pas d'autre arête partant de $\ell$, le joueur n'a donc pas d'autre choix.

L'instruction $0$-test est plus délicate, car le \jo est susceptible de tricher en empruntant la mauvaise branche de l'instruction conditionnelle.
De manière formelle, on suppose que $\ell$ est « si $c=0$ alors aller à l'étiquette~ $\ell_1$, sinon décrémenter $c$ et aller à l'étiquette~$\ell_2$ ».
Il y a alors deux arêtes sortant de l'état $\ell$: l'une correspondant à l'annonce $c=0$, l'autre à l'annonce $c>0$, comme indiqué sur la \figref{tj:fig:zerotest}.
L'arête portant l'affirmation $c=0$ atteint un état possédé par le \ji, qui peut soit accepter l'affirmation (et aller dans ce cas à $\ell_1$), soit la rejeter.
Le rejet de cette affirmation implique de décrémenter $c'$ (mais non $c$) et de se rendre sur un état d'\emph{arrêt}: cet état possède une unique arête sortante, qui boucle sur lui-même avec les poids $(0,0,0,0,0)$, signifiant que la simulation s'est arrêtée.
Si l'affirmation est rejetée à tord, on obtient $c'<0$ tandis que $c=0$, puisque les valeurs de $c$ et $c'$ étaient égales.
Si l'affirmation a été rejetée à raison, alors le gain est nul.

\begin{figure}
\centering
\begin{tikzpicture}[auto,thick,>=stealth,xscale=1.05]
\tikzstyle{stateComp}=[state, shape=rectangle]
\tikzstyle{stateNorm}=[state,inner sep=0pt]
\tikzstyle{mini}=[font=\small]

\node[stateComp] (l0) at (0,0) {$\ell$};
\node[stateNorm] (c0) at (3,0) {`$c=0$'};
\node[stateComp] (l1) at (6,0) {$\ell_1$};
\node[stateComp] (stop1) at (6,-2) {\emph{stop}};
\node[stateNorm] (cpos) at (0,-2) {`$c>0$'};
\node[stateComp] (l2) at (0,-4) {$\ell_2$};
\node[stateComp] (stop2) at (3,-4) {\emph{stop$'$}};

\draw[->] (l0) edge (c0);
\draw[->] (c0) edge (l1);
\draw[->] (c0) edge[bend right] node[mini] {$(0,-1,0,0,0)$} (stop1);

\draw[->] (l0) edge node[mini] {$(-1,-1,0,0,0)$} (cpos);
\draw[->] (cpos) edge (l2);
\draw[->] (cpos) edge (stop2);

\draw[->] (stop1) edge [loop below] (stop1);
\draw[->] (stop2) edge [loop right] node [mini] {$(0,0,0,0,1)$} (stop2);

\end{tikzpicture}
\caption[Module d'encodage de l'instruction $0$-test sous forme de jeu.]{Module d'encodage de l'instruction $0$-test sous forme de jeu: « si $c=0$ alors aller à l'étiquette~$\ell_1$, sinon décrémenter $c$ et aller à l'étiquette~$\ell_2$ ». Les états sous forme de carrés appartiennent au \jo, les cercles représentent les états du \ji. Il est convenu qu'en l'absence d'indication de poids, les arêtes comportent les poids~$(0,0,0,0,0)$.}
\label{tj:fig:zerotest}
\end{figure}

Similarly, the claim that $c>0$ is made by an edge decrementing both $c$ and $c'$ (hence $(-1,-1,0,0,0)$), and going to state of player~1.
He can either follow with the simulation or go to a \emph{stop$'$} state.
This state has a single edge to itself labeled $(0,0,0,0,1)$, meaning the simulation has stopped but the payoff is $1$.
If player~0 cheated, then $c<0$ and the simulation need not go further.
If he didn't, then both $c>0$ and $c'>0$, and $p>1$.

Finally, upon reaching the \emph{halt} state, player~0 decreases both $c$ and $c'$ at will, then go to a \emph{check} state.
State \emph{check} belongs to player~1, and behaves as in the case of $0$-test where player~0 claimed that $c=0$.
The same is also done for $d$, and finally an edge goes back from \emph{check} to the initial state giving a reward to the $p$ component: $(0,0,0,0,1)$.
This module is depicted in \figref{tj:fig:halt}.

\begin{figure}
\centering
\begin{tikzpicture}[auto, thick,>=stealth]
\tikzstyle{stateComp}=[state, shape=rectangle]
\tikzstyle{stateNorm}=[state,inner sep=0pt]
\tikzstyle{mini}=[font=\small]

\node[stateComp] (h) at (0,0) {\emph{halt}};
\node[stateNorm] (z) at (3,0) {\emph{check}};
\node[stateComp] (stop) at (3,-1.5) {\emph{stop}};
\node[stateComp] (l0) at (6,0) {$\ell_0$};

\draw[->] (h) edge[loop above] node[mini] {$(-1,-1,0,0,0)$} (h);
\draw[->] (h) edge[loop below] node[mini] {$(0,0,-1,-1,0)$} (h);
\draw[->] (h) edge (z);

\draw[->] (z) edge[bend right] node[mini,swap,pos=0.55] {$(0,-1,0,0,0)$} (stop);
\draw[->] (z) edge[bend left] node[mini,pos=0.55] {$(0,0,0,-1,0)$} (stop);
\draw[->] (z) edge node[mini] {$(0,0,0,0,1)$} (l0);

\draw[->] (stop) edge [loop right] (stop);

\end{tikzpicture}
\caption[Module for encoding the halting instruction into a game.]{Module for encoding the halting instruction into a game. Square states belong to player~0, round states to the second one. Unmarked edges are assumed to have $(0,0,0,0,0)$ weights.}
\label{tj:fig:halt}
\end{figure}

Now assume that $\machine$ halts.
Consider the strategy of player~0 that plays faithfully \emph{i.e.} claims exactly what the counter value tells (and in the end decrements components so that they exactly reach $0$).
Then if player~1 never claims wrongdoing, counters $c$ and $d$ remain positive as in $\machine$.
In addition, assume $\machine$ finished in $k$ steps and let $c_k$ and $d_k$ be the values of $c$ and $d$, respectively, upon reaching the halting instruction.
Then the run of the machine is then at most $2k$ steps in the game (because of the claims that must be accepted), then from the \emph{halt} state the game needs $c_k+d_k+2$ steps to reach the initial state again: it is the number of steps required in order to decrement both counters and traverse the module of \figref{tj:fig:halt}.
Hence the mean payoff is at least $\frac1{2k+c_k+d_k+2}>0$.

If player~1 rejected a claim:
\begin{itemize}
\item if $c=c'=0$ but player~1 decremented $c'$ to contest the claim, then the game ``stops'' but condition $c'<0 \wedge c\geq0$ is satisfied;
\item if $c=c'\geq0$ after decrementation but the game was ``stopped'', payoff is $1$ while both $c$ and $d$ are above $0$, hence condition $c\geq0 \wedge d\geq0 \wedge p>0$ is satisfied.
\end{itemize}

As a result, this faithful simulation strategy is winning.

\bigskip

On the other hand if $\machine$ does not halt, faithful simulation yields a payoff of $0$ provided player~1 never wrongfully attacks a claim (the halt state is never reached, and if player~1 does not claim cheating, stop$'$ is not reached either).
Note that in case of unfaithful simulation from player~0, player~1 only needs to detect this fault: either $c<0$ or the payoff is irremediably null.

\bigskip

This construction can be adapted to the initial credit problem: one needs to add a module before the initial state that resembles the \emph{halt} module, in the sense that it ensures bringing all energy levels to $0$.
Hence any initial credit is irrelevant and player~0 wins if and only if $\machine$ halts.

\subsection{The positive fragment}

We now consider the initial credit problem.
It is ``easier'' for the compromised node to win for this game rather than for the corresponding winning game.
Additionally, it is common when combining strategies that an initial credit required to win is increased; in other words, strategies for the winning problem are usually not robust enough.
From the model point of view, solving this problem yields more information: if a strategy and a credit are found, they give an actual value to which the initial energy level must be set.

\bigskip

We show in this section that winning strategies for objectives defined by literals may be combined into a winning strategy for the whole objective.

First, the case of a literal is the well-studied case of either energy games or mean-payoff games.
These games have simple solutions, in the sense that (1) these games are determined, \ie one of the player has a winning strategy; (2) if a winning strategy exists, there exists one with finite memory; (3) which player wins the game can be decided in $\NP\cap \coNP$~\cite{zwick96}.
Moreover, it was proved in~\cite{velner12a} that finite memory suffice to win a conjunction of energy requirements.
The case of conjunction of payoff requirement has also been studied in~\cite{velner12a}, where the memoryless strategies for each payoff condition is combined into infinite memory strategies for the whole objective.
For example, when mean-payoff is defined with the superior limit: each strategy is played in increasingly longer phases until reaching the desired value (as close as needed).

As a result we here focus on mixing energy and payoff objectives.
Remark that since the objective of the compromised node is a conjunction of literals, it is clear that if the objective specified by one of the literals cannot be achieved, then the conjunction cannot, hence the game cannot be won.

If there are strategies for each of the objectives, it may be possible to combine finite-memory winning strategies for each of the literals into a single winning strategy, possibly needing infinite memory.
We provide sufficient conditions to do so; finding an algorithm for this problem is still open.

\subsubsection{Attractors}

In two-player games, it is common to use the notion of \emph{attractor} of a set $Q$ to denote states where a player can force the play to reach $Q$.

\begin{definition}
The 1-step \emph{attractors} for players~$0$ and~$1$ of a set $Q$ of states are defined as follows:
\begin{eqnarray*}
1\!\attractor{0}(Q) &=& \left\{q \in V_0 \mmid \exists (q,q') \in E \textrm{ s.t. } q' \in Q \right\}
\\&&\quad
\cup \;\left\{q \in V_1 \mmid \forall (q,q') \in E, q' \in Q \right\}
\end{eqnarray*}
\begin{eqnarray*}
1\!\attractor{1}(Q) &=& \left\{q \in V_0 \mmid \forall (q,q') \in E, q' \in Q \right\}
\\&&\quad
\cup \;\left\{q \in V_1 \mmid \exists (q,q') \in E \textrm{ s.t. } q' \in Q \right\}
\end{eqnarray*}
The \emph{attractors} for player~$0$ (resp. player~$1$) of a set $Q$ of states are the fix-point of the 1-step attractor, starting from $Q$: for $i \in \{0,1\}$,
\[\attractor{i}(Q) = \bigcup_{j \in \N} \left(1\!\attractor{i}\right)^j(Q)\]
%One can also define a strict version of this attractor, which excludes the set $Q$ itself (except if $q \in Q$ can reach $Q$):
%\[\attractor{i}^+(Q) = \bigcup_{j \in \N\setminus\{0\}} \left(1\!\attractor{i}\right)^j(Q)\]
\end{definition}
The attractor of player~$i$ is therefore the set of states from which he can ensure that the play reaches $Q$.
Note that from any state in $(1\!\attractor{i})^j(Q)$, player $i$ has a memoryless strategy to reach $Q$ in at most $j$ steps.
Since the fix-point is reached in at most $|V|$ iterations, $Q$ can be reached in at most $|V|$ steps from any state of $\attractor{i}(Q)$.
Note that this bound also shows that attractors can be computed in polynomial time.

\begin{lemma}
From any state of $\attractor{i}(Q)$, player $i$ has a memoryless strategy that ensures reaching $Q$ in at most $|V|$ steps.
\end{lemma}

A property of attractors in games is that they can be ``safely'' removed from a game while leaving the graph structure still a game (\emph{i.e.} without end-states):
\begin{lemma}\label{tj:lem:removeattr}
Let $\G = \langle V_0, V_1, E\rangle$ be a game graph (\emph{i.e.} such that every state of $V$ has an outgoing edge in $E$).
Let $j \in \{0,1\}$ be a player.
Let $Q \subseteq V$ and consider the graph $\G' = (V_0', V_1', E \cap (V' \times V'))$ with $V_i' = V_i \setminus \attractor{j}(Q)$ for $i \in \{0,1\}$.
Then every state of $V'$ has an outgoing edge in $E \cap (V' \times V')$, \emph{i.e.} $\G'$ is also a game graph.
\end{lemma}

\begin{proof}
Assume by contradiction that $q \in V'$ has no outgoing edge.
Since $q$ had an outgoing edge in $E$, it means that all successor states of $q$ belong to $\attractor{j}(Q)$.
Then by definition of an attractor, so does $q$, and $q \notin V'$.
\end{proof}

\subsubsection{Conjunction of an energy and payoff objective}

In this first simpler case, we study objectives for player~0 of the form $p_e \geq c_e \wedge p_v \geq c_v$, with $c_e,c_v \in \N$.
These objectives state that a certain reward must be achieved while maintaining the energy level above a given limit.
A simple example of this kind of objective for a compromised node is the \emph{greedy} objective: to remain alive ($p_e \geq 1$) while sending at least a message every $6$ steps ($p_v \geq \frac1{6}$, as noted before, this can be transformed into an integral threshold by multiplying each weight of this component by $6$).

First, it is clear that from states where one of the objective cannot be fulfilled, player~0 cannot win.
In addition, if player~1 can force to reach such states, then the objective of player~0 cannot be fulfilled.
Namely, we use the classical notion of \emph{attractors} defined above.
We write $L_e$ the states where player~0 loses for $p_e \geq c_e$ (\emph{i.e.} player~1 has a strategy to prevent that), and $L_v$ the states where player~0 loses for $p_v \geq c_v$.
It is clear that player~1 can prevent player~0 from winning for objective $p_e \geq c_e \wedge p_v \geq c_v$ from any state in $\attractor{g}(L_e \cup L_v)$.
Hence any winning strategy for player~0 must remain in $\G \setminus\attractor{1}(L_e \cup L_v)$.
Conversely, a winning strategy that remains in $\G \setminus\attractor{1}(L_e \cup L_v)$ is also a winning strategy in $\G$ since player~1 cannot force the play into $\attractor{1}(L_e \cup L_v)$, by the definition of attractors.

As a result, one can recursively remove states in $\G$ until player~0 wins for both objectives in every state.
Note that if the game is empty at that point, then player~0 cannot win from any state.

Now assume player~0 has winning strategies $\lambda_e$, $\lambda_v$ for objectives $p_e \geq 1$, $p_v \geq \frac1{15}$, respectively, that win from every state.
Remark that these strategies can be assumed to be memoryless, hence functions from $V_c$ to $V$.

Consider $\G_e$ the single player game obtained when player~0 plays $\lambda_e$: each transition entering a state $v \in V_c$ goes instead to $\lambda_e(v)$, and weights are added: $v_0 \xrightarrow{w_1} v \xrightarrow{w_2} \lambda_e(v)$ is replaced by $v_0 \xrightarrow{w_1+w_2} \lambda_e(v)$.
Similarly, let $\G_v$ the single player game obtained when playing $\lambda_v$.

Let $\alpha$ be the lowest value that can be obtained for component $k_v$ in a simple cycle in $\G_e$, and dually $\beta$ the lowest value that can be obtained for $k_e$ in $\G_v$.
In other words, $\alpha$ is the worst that can happen to reward when playing the strategy ensuring adequate level of energy, while $\beta$ is the worst that can happen to energy level when ensuring adequate reward.
Note that if $\alpha \geq c_v$, then $\lambda_e$ is also a winning strategy for objective $p_v \geq c_v$, hence is a winning strategy for the whole objective.
Similarly, if $\beta > 0$, then $\lambda_v$ ensures objective $p_e \geq c_e$ provided the initial credit is adapted to the minimal sum of weights reached along a cycle:
\[\textit{initial\_credit} = 1+c+\min_{\substack{\rho \textrm{ prefix of } \mathcal{C}\\  \mathcal{C}\textrm{ cycle in } \G_v}} w_e(\rho)\]

\medskip

If the above sufficient condition is not fulfilled, we do not so far have a solution for solving these games in the general setting.
Indeed, strategies for each component or the other can be incompatible.
For example, consider the (single-player) game of \figref{tj:fig:infmemoryneeded}.
In this example, $q_0$ acts as a recharging state while $q_1$ is an active state, producing a useful effect rewarded by a payoff.
One can see that recharging the battery is much slower than using it, since it takes 42 ``energy units'' per active step\footnote{For example this can be a joule, meaning that the active state consumes 42W of power while the charging is done with 1W of power. The value 42 was arbitrarily chosen.}

One can achieve a positive energy (first component) at all times by remaining in $q_0$ (or going there if starting from $q_1$).
It is also possible to achieve a payoff (second component) of $1$ by remaining in $q_1$ (or going there if starting from $q_0$, the transitive effect is negligible in the long run).
However, one cannot achieve a payoff of $1$ while maintaining the energy positive, since it takes 42 turns of ``recharging'' before being allowed to do something rewarding.

\begin{figure}
\centering
\begin{tikzpicture}[auto, thick,>=stealth]
\tikzstyle{stateComp}=[state, shape=rectangle]

\node[stateComp] (q0) at (0,0) {$q_0$};
\node[stateComp] (q1) at (3,0) {$q_1$};

\path[->] (q0) edge[loop left] node {$+1,0$} (q0);
\path[->] (q0) edge[bend left=15] node {$0,0$} (q1);
\path[->] (q1) edge[loop right] node {$-42,+1$} (q1);
\path[->] (q1) edge[bend left=15] node {$0,0$} (q0);
\end{tikzpicture}
\caption{A simple game that requires infinite memory. The first component is an energy level while the second is a payoff.}
\label{tj:fig:infmemoryneeded}
\end{figure}

\subsubsection{Using bounded memory}

However, one can consider that nodes of a \wsn are very limited in their resources, hence can only implement finite memory strategies.
In this case, bounding \emph{a priori} the amount of memory that can be used by player provides a solution for solving the initial credit problem for games with winning condition in the positive fragment.

A finite memory strategy is a strategy that can be implemented by a finite deterministic Mealy machine: given the current state of the machine and of the game, the machine produces an edge to be played and the next state of the machine.
The size of the memory is the size of the Mealy machine.

For example, a finite memory strategy for the game of \figref{tj:fig:infmemoryneeded} that loops 42 times in $q_0$ then goes to $q_1$, loops once there, and goes back to $q_0$ to start again can be represented by the machine of \figref{tj:fig:memory42}.
This machine has 46 states, since it needs to count how many times it has accumulated energy in $q_0$.
Note that if starting in $q_1$, the machine first goes back to $q_0$ then applies the aforementioned strategy.
Also, a strategy must be complete on its input, hence it must allow $q_1$ to occur in any memory state (in this case it goes back to the initial memory state and to $q_0$ in the game).

\begin{figure}
\centering
\begin{tikzpicture}[auto, thick,>=stealth]
\useasboundingbox (-1.25,1.45) rectangle (7.8,-3.15);
\node[state, initial,initial text=] (m0) at (0,0) {$m_0$};
\node[state] (m1) at (2,0) {$m_1$};
\node (dots) at (3.5,0) {\dots};
\node[state] (m42) at (5,0) {$m_{42}$};
\node[state] (m43) at (7,0) {$m_{43}$};
\node[state] (m44) at (7,-2) {$m_{44}$};
\node[state] (m45) at (5,-2) {$m_{45}$};

\draw[->] (m0) edge[loop above] node {$q_1 | q_0$} (m0);
\draw[->] (m0) edge node {$q_0 | q_0$} (m1);
\draw[->] (m1) edge node {$q_0 | q_0$} (dots);
\draw[->] (m42) edge node {$q_0 | q_0$} (m43);
\draw[->] (m43) edge node {$q_0 | q_1$} (m44);
\draw[->] (m44) edge node {$q_1 | q_1$} (m45);
\draw[->] (m45) edge[bend left] node[swap,pos=0.15] {$q_1 | q_0$} (m0);

\draw[->] (m1) edge[bend left=15] node[pos=0.15] {$q_1 | q_0$} (m0);
\draw[->] (m42) edge[bend left=25] node[,pos=0.15] {$q_1 | q_0$} (m0);
\draw[->] (m43) edge[bend left=45] node[swap,pos=0.33] {$q_1 | q_0$} (m0);
\draw[->] (m44) edge[bend left=60, in =60] node[pos=0.75] {$q_0 | q_0$} (m0);
\draw[->] (m45) edge[bend left=60] node[pos=0.1] {$q_0 | q_0$} (m0);
\end{tikzpicture}
\caption[A Mealy machine representing a finite memory strategy.]{A Mealy machine representing a finite memory strategy (46 states). The input of edges is the current state of the game, the output is the chosen next state.}
\label{tj:fig:memory42}
\end{figure}

\begin{remark}
Note that the strategies with given memory may not be optimal.
Consider for example the game of \figref{tj:fig:infmemoryneeded} with objective being to maintain the energy level above $0$ and ensure a mean-payoff greater than or equal to $\frac1{43}$.

An infinite memory strategy can win this game.
It is defined by phases, as follows.
At the $k$-th phase, loop $42k$ times in $q_0$, then go to $q_1$, loop $k$ times, go back to $q_0$.
This ensures that the transitions between $q_0$ and $q_1$ are negligible, hence the limit of the payoff is $\frac1{43}$, since 43 steps are needed to increment the payoff counter once.
In addition, at each phase the energy goes back to its initial value (which can be $1$), while only encountering positive values.

On the other hand, if only $k$ states of memory are allowed, the best payoff achievable is $\frac{k}{43k+2}$ (the corresponding strategy consists in repeating phase $k$).
Hence not only the optimal payoff cannot be achieved, but allowing more memory allows to achieve better payoff.
\end{remark}

Solving the game assuming player~0 has bounded memory $k$ given as input consists in guessing this strategy as a machine, which is an exponential object if $k$ is given in binary.
Then the machine is synchronized with the game, yielding a single-player game, to be played by player~1.
In this game, only memoryless strategies need to be considered.

Indeed, any infinite path not satisfying $p_e \geq c_e \wedge p_v \geq c_v$ either is such that $p_e$ falls below $c_e$ or the limit average of $p_v$ is below $c_v$.
In both cases this amounts to finding a lasso path in the graph, which can be guessed.

Regarding complexity, the above procedure is in $\NEXPSPACE$ (which is equivalent to $\EXPSPACE$~\cite[Chap.~20]{papadimitriou94}), although the bound is not tight.
