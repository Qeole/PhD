% vim: set spelllang=fr,en foldmethod=marker:
\section{Résolution des jeux}
\label{tj:sec:algo}

Cette section s'intéresse au problème posé par la résolution de ces jeux.
Nous montrerons tout d'abord que la résolution est un problème indécidable lorsque la condition de victoire est une formule de gain arbitraire.
Nous nous concentrerons ensuite sur le segment positif de ces formules, et donnerons des conditions suffisantes pour pouvoir résoudre le problème.

\newcommand\jo{joueur~0\xspace}
\newcommand\ji{joueur~1\xspace}
Afin de clarifier la présentation, nous considérons dans cette section deux joueurs, simplement désignés par les termes \jo et \ji.
Le \jo est celui qui cherche à atteindre l'objectif décrit par la formule de gain, et représente donc le nœud compromis.
Conformément à la convention adoptée dans la section précédente, ses états seront représentés sous forme de rectangles.
Le \ji quant à lui représente l'« équipe » (l'ensemble) des autres nœuds, et ses états seront de forme circulaire.

\subsection{Indécidabilité du cas général}

Nous allons démontrer ici que le problème de victoire pour les jeux étudiés ne présente pas de solution algorithmique si la condition de victoire ne peut pas être spécifiée par une formule de gain.
Le théorème établi est le suivant:
\begin{theorem}
Le problème de victoire, ainsi que le problème de crédit initial, sont indécidables pour des objectifs définis par une formule de gain comportant quatre composantes relatives à l'énergie et une composante relative au gain.
\end{theorem}

La preuve de ce théorème consiste à encoder, à l'aide d'un jeu du type étudié, le problème de terminaison à l'aide d'une machine à deux compteurs: le problème équivalent ainsi obtenu est connu comme étant indécidable\cite{minsky67}.
Chaque compteur est représenté par deux composantes d'énergie (une pour chacun des deux joueurs), qui contiennent chacune une copie de la valeur du compteur en question.
Pour l'instruction $0$-test (présentée ci-dessous), le \jo peut prétendre que son compteur est nul, ou bien qu'il ne l'est pas; le second joueur peut vérifier si cette affirmation est valide ou non.
Si la machine atteint son état d'arrêt, une composante de gain est incrémentée, et tous les autres composantes sont réinitialisées à $0$.
De cette manière, si la machine s'arrête, le \jo a une stratégie gagnante lui permettant d'obtenir un gain strictement positif.
Autrement, le gain demeure nul.

On définit plus précisément une machine à deux compteurs comme une liste d'instructions étiquetées, qui peuvent être:
\begin{itemize}
    \item \textbf{increment:} incrémenter le compteur $c$ (respectivement $d$) et aller à l'étiquette~$\ell$
    \item \textbf{0-test:} si $c=0$ (respectivement $d=0$) alors aller à l'étiquette~$\ell_1$, sinon décrémenter $c$ (respectivement $d$) et aller à l'étiquette~$\ell_2$
    \item \textbf{halting:} \emph{arrêt}
\end{itemize}
Au départ, les deux compteurs sont initialisés à $0$.

À présent, étant donné une machine $\machine$, nous construisons une arène $\G_\machine$ basée sur des modules « élémentaires » pour chaque type d'instructions (celui du compteur $c$ sera symétrique à celui du compteur $d$).
L'arène comporte donc 5 composantes $c$,$c'$,$d$,$d'$,$p$, où seule $p$ concerne le gain, les autres étant attachées à des valeurs énergétiques.
Les composantes $c$ et $c'$ (respectivement $d$ et $d'$) sont supposées toujours conserver la même valeur, excepté pour l'instruction $0$-test.
Au lancement du jeu toutes les composantes ont pour valeur\footnote{Pour obtenir une simulation plus réaliste, il est possible d'assigner aux composantes la valeur initiale $1$, et de considérer $0$ comme valeur de défaite. Nous choisissons ici d'autoriser un niveau d'énergie nul et considérons le jeu comme « perdu » pour des valeurs strictement négative, et ce afin de rendre plus clair le lien entre les valeurs des compteurs et les niveaux d'énergie.} $0$.

Le but du \jo est d'obtenir un gain strictement positif, tout en simulant fidèlement la machine.
Il faut entendre par cette expression que les valeurs des composantes $c$ et $d$ doivent demeurer positives.
Les composantes $c'$ et $d'$ permettent de s'assurer que le \ji ne pourra prétendre à tord que le \jo a triché.
Si le cas devait se produire, les valeurs de ces composantes seraient strictement négatives.
À présent la condition de victoire s'exprime comme la satisfaction de la formule de gain suivante:
\[(c\geq0 \wedge d\geq0 \wedge p>0) \vee (c'<0 \wedge c\geq0) \vee (d'<0 \wedge d\geq0)\]

Chaque instruction étiquetée a un état dans l'arène appartenant au \jo, avec la même étiquette (d'autres états seront rajoutés par la suite).
Le cas de l'incrément est trivial: si l'instruction $\ell$ est « incrémenter $c$ puis aller à l'étiquette~$\ell'$ », alors il y a une arête partant de $\ell$ et joignant $\ell'$ avec les poids $(1,1,0,0,0)$.
Il n'y a pas d'autre arête partant de $\ell$, le joueur n'a donc pas d'autre choix.

L'instruction $0$-test est plus délicate, car le \jo est susceptible de tricher en empruntant la mauvaise branche de l'instruction conditionnelle.
De manière formelle, on suppose que $\ell$ est « si $c=0$ alors aller à l'étiquette~ $\ell_1$, sinon décrémenter $c$ et aller à l'étiquette~$\ell_2$ ».
Il y a alors deux arêtes sortant de l'état $\ell$: l'une correspondant à l'annonce $c=0$, l'autre à l'annonce $c>0$, comme indiqué sur la \figref{tj:fig:zerotest}.
L'arête portant l'affirmation $c=0$ atteint un état possédé par le \ji, qui peut soit accepter l'affirmation (et aller dans ce cas à $\ell_1$), soit la rejeter.
Le rejet de cette affirmation implique de décrémenter $c'$ (mais non $c$) et de se rendre sur un état d'\emph{arrêt}: cet état possède une unique arête sortante, qui boucle sur lui-même avec les poids $(0,0,0,0,0)$, signifiant que la simulation s'est arrêtée.
Si l'affirmation est rejetée à tord, on obtient $c'<0$ tandis que $c=0$, puisque les valeurs de $c$ et $c'$ étaient égales.
Si l'affirmation a été rejetée à raison, alors le gain est nul.

\begin{figure}
\centering
\begin{tikzpicture}[auto,thick,>=stealth,xscale=1.05]
\tikzstyle{stateComp}=[state, shape=rectangle]
\tikzstyle{stateNorm}=[state,inner sep=0pt]
\tikzstyle{mini}=[font=\small]

\node[stateComp] (l0) at (0,0) {$\ell$};
\node[stateNorm] (c0) at (3,0) {`$c=0$'};
\node[stateComp] (l1) at (6,0) {$\ell_1$};
\node[stateComp] (stop1) at (6,-2) {\emph{stop}};
\node[stateNorm] (cpos) at (0,-2) {`$c>0$'};
\node[stateComp] (l2) at (0,-4) {$\ell_2$};
\node[stateComp] (stop2) at (3,-4) {\emph{stop$'$}};

\draw[->] (l0) edge (c0);
\draw[->] (c0) edge (l1);
\draw[->] (c0) edge[bend right] node[mini] {$(0,-1,0,0,0)$} (stop1);

\draw[->] (l0) edge node[mini] {$(-1,-1,0,0,0)$} (cpos);
\draw[->] (cpos) edge (l2);
\draw[->] (cpos) edge (stop2);

\draw[->] (stop1) edge [loop below] (stop1);
\draw[->] (stop2) edge [loop right] node [mini] {$(0,0,0,0,1)$} (stop2);

\end{tikzpicture}
\caption[Module d'encodage de l'instruction $0$-test sous forme de jeu.]{Module d'encodage de l'instruction $0$-test sous forme de jeu: « si $c=0$ alors aller à l'étiquette~$\ell_1$, sinon décrémenter $c$ et aller à l'étiquette~$\ell_2$ ». Les états sous forme de carrés appartiennent au \jo, les cercles représentent les états du \ji. Il est convenu qu'en l'absence d'indication de poids, les arêtes comportent les poids~$(0,0,0,0,0)$.}
\label{tj:fig:zerotest}
\end{figure}

De même, l'affirmation $c>0$ est représentée par une arête qui décrémente à la fois $c$ et $c'$ (donc de poids $(-1,-1,0,0,0)$), et atteint l'état du \ji.
Ce dernier peut soit poursuivre la simulation, soit se rendre sur un état \emph{arrêt$'$}.
Cet état possède une unique arête qui boucle sur lui-même avec les poids $(0,0,0,0,1)$, ce qui signifie que la simulations est arrêtée mais que le gain vaut $1$.
Si le \jo a triché, alors $c<0$ et la simulation n'a pas besoin d'être menée plus avant.
S'il s'est comporté de façon loyale, alors $c>0$ et $c'>0$, et $p>1$.

Enfin, lorsqu'il atteint l'état d'\emph{arrêt}, le \jo décrémente à la fois $c$ et $c'$ autant de fois qu'il le souhaite, puis se rend sur un état de vérification (\emph{check}).
L'état \emph{check} appartient au \ji, et agit de la même façon que pour le $0$-test dans le cas où le \jo affirme que $c=0$.
On procède de façon identique pour $d$.
Enfin, une arête retourne depuis l'état \emph{check} jusqu'à l'état initial et rapporte une récompense à la composante de gain~$p$: son poids est $(0,0,0,0,1)$.
Ce module est représenté sur la \figref{tj:fig:halt}.
\todo{Remettre états \emph{halt} / \emph{stop}}

\begin{figure}
\centering
\begin{tikzpicture}[auto, thick,>=stealth]
\tikzstyle{stateComp}=[state, shape=rectangle]
\tikzstyle{stateNorm}=[state,inner sep=0pt]
\tikzstyle{mini}=[font=\small]

\node[stateComp] (h) at (0,0) {\emph{halt}};
\node[stateNorm] (z) at (3,0) {\emph{check}};
\node[stateComp] (stop) at (3,-1.5) {\emph{stop}};
\node[stateComp] (l0) at (6,0) {$\ell_0$};

\draw[->] (h) edge[loop above] node[mini] {$(-1,-1,0,0,0)$} (h);
\draw[->] (h) edge[loop below] node[mini] {$(0,0,-1,-1,0)$} (h);
\draw[->] (h) edge (z);

\draw[->] (z) edge[bend right] node[mini,swap,pos=0.55] {$(0,-1,0,0,0)$} (stop);
\draw[->] (z) edge[bend left] node[mini,pos=0.55] {$(0,0,0,-1,0)$} (stop);
\draw[->] (z) edge node[mini] {$(0,0,0,0,1)$} (l0);

\draw[->] (stop) edge [loop right] (stop);

\end{tikzpicture}
\caption[Module d'encodage des instructions d'arrêt sous forme de jeu.]{Module d'encodage des instructions d'arrêt sous forme de jeu. Les états sous forme de carrés appartiennent au \jo, les cercles représentent les états du \ji. Il est convenu qu'en l'absence d'indication de poids, les arêtes comportent les poids~$(0,0,0,0,0)$.}
\label{tj:fig:halt}
\end{figure}

Supposons à présent que la \machine s'arrête.
Considérons la stratégie du \jo qui consiste à jouer honnêtement, \cad à annoncer les valeurs correctes du compteur (et à décrémenter les composantes d'énergie, de sorte qu'elles atteignent exactement $0$).
Alors si le \ji ne lance aucune accusation de tricherie, les compteurs $c$ et $d$ restent tous deux positifs, comme pour la \machine\todo{Read again}.
Supposons de plus que la \machine a terminé son exécution en $k$ étape, et dénotons par $c_k$ et $d_k$ les valeurs respectives de $c$ et $d$ lorsque l'on atteint les instructions d"arrêt.
Dans ce cas l'exécution de la simulation sur la machine correspond au plus à $2k$ étapes du jeu (à cause des affirmations qui doivent être acceptées), et le jeu nécessite $c_k+d_k+2$ étapes depuis l'état \emph{halt} pour atteindre à nouveau l'état initial: il s'agit du nombre d'étapes requises pour décrémenter chaque compteur et pour traverser le module de la \figref{tj:fig:halt}.
En conséquence la valeur minimale du gain moyen est de $\frac1{2k+c_k+d_k+2}>0$.

Si le \ji a rejeté l'une des affirmations du \jo:
\begin{itemize}
\item si $c=c'=0$, mais que le \ji a décrémenté $c'$ pour contester l'annonce, alors le jeu « s'arrête », mais la condition~$c'<0 \wedge c\geq0$ est satisfaite
\item si $c=c'\geq0$ après le décrément mais que le jeu a été « arrêté », le gain vaut $1$ tandis que les deux composantes $c$ et $d$ sont supérieures à $0$. Dans ce cas la condition~$c\geq0 \wedge d\geq0 \wedge p>0$ est satisfaite.
\end{itemize}

Au final, on retient que la stratégie de simulation « loyale » est gagnante.

\bigskip

Supposons maintenant que la \machine ne s'arrête pas: la simulation « loyale » retourne une valeur de gain de $0$ si le \ji ne porte aucune accusation à tord (l'état d'arrêt \emph{halt} n'est jamais atteint, et si le \ji ne réfute aucune annonce, l'état \emph{stop$'$} n'est pas non plus atteint.)
Dans le cas où le \jo « ment » lors de ses annonces, il suffit au \ji de détecter cette erreur pour que l'on obtienne soit $c<0$, soit une valeur de gain définitivement nulle.

\bigskip

Cette construction peut être adaptée au problème du crédit initial: il suffit seulement de rajouter, en amont de l'état initial, un module similaire au module \emph{halt} dans le sens où il doit permettre à tous les niveaux d'énergie d'atteindre $0$.
Une fois l'état « initial » (à l'origine) atteint, le crédit initial devient caduque, et le \jo gagne si et seulement si la \machine s'arrête.

\subsection{Le fragment positif}

Revenons à présent sur le problème du crédit initial, dont le jeu est plus « simple » à remporter pour le nœud compromis que le jeu du problème de victoire correspondant.
De plus il est courant, lorsque l'on combine des stratégies, qu'un crédit initial nécessaire à la victoire soit augmenté; en d'autres termes, les stratégies concernant le problème de victoire ne sont souvent pas assez robustes.
Du point de vue du modèle employé, résoudre le problème du crédit initial permet d'obtenir plus d'informations: si une stratégie et un crédit sont découverts, ils fournissent une valeur concrète à laquelle l'énergie doit être initialisée.

\bigskip

Nous démontrons dans cette section que les stratégies gagnantes pour les objectifs définis par des littéraux peuvent être combinés en une stratégie gagnante pour l'objectif global.

Rappelons avant tout que le cas d'un littéral a été étudié en détail, pour des jeux relatifs soit à de l'énergie, soit à des gains moyen.
Ces jeux ont une solution simple, dans le sens où:
\begin{itemize}
    \item ces jeux sont déterministes\todo{determined}, autrement dit l'un des joueurs a une stratégie gagnante;
    \item si une stratégie gagnante existe, alors il en existe une qui repose sur une mémoire finie;
    \item déterminer quel sera le joueur vainqueur peur être réalisé en $\NP\cap \coNP$~\cite{zwick96}.
\end{itemize}
De plus, il a été prouvé~\cite{velner12a} qu'une mémoire finie est suffisante pour gagner sur une conjonction de contraintes sur l'énergie.
Le cas des conjonctions de contraintes sur le gain a lui aussi fait l'objet d'études dans~\cite{velner12a}, où les stratégies sans mémoire pour chaque condition sur le gain sont combinées en stratégies à mémoire infinie pour l'objectif global.
Par exemple, lorsque le gain moyen des défini par la limite supérieur du gain, chaque stratégie peut être jouée au cours de phases de durée croissante jusqu'à atteindre (ou à s'approcher aussi près que nécessaire de) la valeur désirée.

Par conséquent nous allons nous concentrer ici sur l'association d'objectifs portant à la fois sur l'énergie et sur le gain moyen.
Il est bon de noter avant tout que si l'objectif du nœud compromis est une conjonction de littéraux, et que l'un deux ne peut être réalisé, il est évident que le jeu ne peut être remporté.

S'il existe des stratégies pour chacun des sous-objectifs, il peut être possible de combiner des stratégies gagnantes à mémoire finie pour chacun des littéraux en une unique stratégie gagnante, nécessitant potentiellement une mémoire infinie.
Nous allons fournir des conditions suffisantes pour obtenir une telle stratégie globale; la recherche algorithmique de la stratégie en question est un problème non résolu.

\subsubsection{Attracteurs}

Dans les jeux à deux joueurs, il est courant de faire appel à la notion d'\emph{attracteurs} d'un ensemble $Q$ pour désigner les états depuis lesquels un joueur peut forcer le jeu à se déplacer dans $Q$.

\begin{definition}
Les \emph{attracteurs une-étape} d'un ensemble d'états $Q$ pour les joueurs~0 et~1 sont définis ainsi:
\begin{eqnarray*}
1\!\attractor{0}(Q) &=& \left\{q \in V_0 \mmid \exists (q,q') \in E \textrm{ s.t. } q' \in Q \right\}
\\&&\quad
\cup \;\left\{q \in V_1 \mmid \forall (q,q') \in E, q' \in Q \right\}
\end{eqnarray*}
\begin{eqnarray*}
1\!\attractor{1}(Q) &=& \left\{q \in V_0 \mmid \forall (q,q') \in E, q' \in Q \right\}
\\&&\quad
\cup \;\left\{q \in V_1 \mmid \exists (q,q') \in E \textrm{ s.t. } q' \in Q \right\}
\end{eqnarray*}
Les \emph{attracteurs} pour le \jo (respectivement pour le \ji) d'un ensemble $Q$ d'étatssont les points fixes des attracteurs une-étape, en partant de $Q$: pour $i \in \{0,1\}$, \[\attractor{i}(Q) = \bigcup_{j \in \N} \left(1\!\attractor{i}\right)^j(Q)\]
\end{definition}
L'attracteur pour le joueur~$i$ est donc l'ensemble des états depuis lesquels ce joueur est en mesure de s'assurer que le jeu atteigne $Q$.
On notera que pour tout état de $(1\!\attractor{i})^j(Q)$, le joueur~$i$ a une stratégie sans mémoire lui permettant d'atteindre $Q$ en $j$ étapes au plus.player $i$ has a memoryless strategy to reach $Q$ in at most $j$ steps.
Puisque le point fixe est atteint en $|V|$ itérations au plus, $Q$ peut être atteint avec un maximum de $|V|$ étapes depuis n'importe quel état de $\attractor{i}(Q)$.
On notera aussi que cette borne entraîne la possibilité de calculer les attracteurs en un temps polynomial.

\begin{lemma}
Depuis n'importe quel état de $\attractor{i}(Q)$, le joueur~$i$ possède une stratégie sans mémoire qui l'assure d'atteindre $Q$ en un maximum de $|V|$ étapes.
\end{lemma}

Une propriété des attracteurs dans les jeux est qu'ils peuvent être retirés sans danger du jeu, tout en laissant la structure du graphe restant toujours associée à un jeu (\cad sans état final):
\begin{lemma}\label{tj:lem:removeattr}
Soit $\G = \langle V_0, V_1, E\rangle$ le graphe d'un jeu (\cad tel que tout état de $V$ a une arête sortante allant dans $E$).
Soit un joueur $j \in \{0,1\}$.
Soit $Q \subseteq V$.
Considérons le graphe $\G' = (V_0', V_1', E \cap (V' \times V'))$ with $V_i' = V_i \setminus \attractor{j}(Q)$ pour $i \in \{0,1\}$.
Alors tout état de $V'$ a une arête sortante allant dans $E \cap (V' \times V')$, autrement dit$\G'$ est aussi un graphe de jeu.
\end{lemma}

\begin{proof}
Assume by contradiction that $q \in V'$ has no outgoing edge.
Since $q$ had an outgoing edge in $E$, it means that all successor states of $q$ belong to $\attractor{j}(Q)$.
Then by definition of an attractor, so does $q$, and $q \notin V'$.
\end{proof}

\subsubsection{Conjunction of an energy and payoff objective}

In this first simpler case, we study objectives for player~0 of the form $p_e \geq c_e \wedge p_v \geq c_v$, with $c_e,c_v \in \N$.
These objectives state that a certain reward must be achieved while maintaining the energy level above a given limit.
A simple example of this kind of objective for a compromised node is the \emph{greedy} objective: to remain alive ($p_e \geq 1$) while sending at least a message every $6$ steps ($p_v \geq \frac1{6}$, as noted before, this can be transformed into an integral threshold by multiplying each weight of this component by $6$).

First, it is clear that from states where one of the objective cannot be fulfilled, player~0 cannot win.
In addition, if player~1 can force to reach such states, then the objective of player~0 cannot be fulfilled.
Namely, we use the classical notion of \emph{attractors} defined above.
We write $L_e$ the states where player~0 loses for $p_e \geq c_e$ (\emph{i.e.} player~1 has a strategy to prevent that), and $L_v$ the states where player~0 loses for $p_v \geq c_v$.
It is clear that player~1 can prevent player~0 from winning for objective $p_e \geq c_e \wedge p_v \geq c_v$ from any state in $\attractor{g}(L_e \cup L_v)$.
Hence any winning strategy for player~0 must remain in $\G \setminus\attractor{1}(L_e \cup L_v)$.
Conversely, a winning strategy that remains in $\G \setminus\attractor{1}(L_e \cup L_v)$ is also a winning strategy in $\G$ since player~1 cannot force the play into $\attractor{1}(L_e \cup L_v)$, by the definition of attractors.

As a result, one can recursively remove states in $\G$ until player~0 wins for both objectives in every state.
Note that if the game is empty at that point, then player~0 cannot win from any state.

Now assume player~0 has winning strategies $\lambda_e$, $\lambda_v$ for objectives $p_e \geq 1$, $p_v \geq \frac1{15}$, respectively, that win from every state.
Remark that these strategies can be assumed to be memoryless, hence functions from $V_c$ to $V$.

Consider $\G_e$ the single player game obtained when player~0 plays $\lambda_e$: each transition entering a state $v \in V_c$ goes instead to $\lambda_e(v)$, and weights are added: $v_0 \xrightarrow{w_1} v \xrightarrow{w_2} \lambda_e(v)$ is replaced by $v_0 \xrightarrow{w_1+w_2} \lambda_e(v)$.
Similarly, let $\G_v$ the single player game obtained when playing $\lambda_v$.

Let $\alpha$ be the lowest value that can be obtained for component $k_v$ in a simple cycle in $\G_e$, and dually $\beta$ the lowest value that can be obtained for $k_e$ in $\G_v$.
In other words, $\alpha$ is the worst that can happen to reward when playing the strategy ensuring adequate level of energy, while $\beta$ is the worst that can happen to energy level when ensuring adequate reward.
Note that if $\alpha \geq c_v$, then $\lambda_e$ is also a winning strategy for objective $p_v \geq c_v$, hence is a winning strategy for the whole objective.
Similarly, if $\beta > 0$, then $\lambda_v$ ensures objective $p_e \geq c_e$ provided the initial credit is adapted to the minimal sum of weights reached along a cycle:
\[\textit{initial\_credit} = 1+c+\min_{\substack{\rho \textrm{ prefix of } \mathcal{C}\\  \mathcal{C}\textrm{ cycle in } \G_v}} w_e(\rho)\]

\medskip

If the above sufficient condition is not fulfilled, we do not so far have a solution for solving these games in the general setting.
Indeed, strategies for each component or the other can be incompatible.
For example, consider the (single-player) game of \figref{tj:fig:infmemoryneeded}.
In this example, $q_0$ acts as a recharging state while $q_1$ is an active state, producing a useful effect rewarded by a payoff.
One can see that recharging the battery is much slower than using it, since it takes 42 ``energy units'' per active step\footnote{For example this can be a joule, meaning that the active state consumes 42W of power while the charging is done with 1W of power. The value 42 was arbitrarily chosen.}

One can achieve a positive energy (first component) at all times by remaining in $q_0$ (or going there if starting from $q_1$).
It is also possible to achieve a payoff (second component) of $1$ by remaining in $q_1$ (or going there if starting from $q_0$, the transitive effect is negligible in the long run).
However, one cannot achieve a payoff of $1$ while maintaining the energy positive, since it takes 42 turns of ``recharging'' before being allowed to do something rewarding.

\begin{figure}
\centering
\begin{tikzpicture}[auto, thick,>=stealth]
\tikzstyle{stateComp}=[state, shape=rectangle]

\node[stateComp] (q0) at (0,0) {$q_0$};
\node[stateComp] (q1) at (3,0) {$q_1$};

\path[->] (q0) edge[loop left] node {$+1,0$} (q0);
\path[->] (q0) edge[bend left=15] node {$0,0$} (q1);
\path[->] (q1) edge[loop right] node {$-42,+1$} (q1);
\path[->] (q1) edge[bend left=15] node {$0,0$} (q0);
\end{tikzpicture}
\caption{A simple game that requires infinite memory. The first component is an energy level while the second is a payoff.}
\label{tj:fig:infmemoryneeded}
\end{figure}

\subsubsection{Using bounded memory}

However, one can consider that nodes of a \wsn are very limited in their resources, hence can only implement finite memory strategies.
In this case, bounding \emph{a priori} the amount of memory that can be used by player provides a solution for solving the initial credit problem for games with winning condition in the positive fragment.

A finite memory strategy is a strategy that can be implemented by a finite deterministic Mealy machine: given the current state of the machine and of the game, the machine produces an edge to be played and the next state of the machine.
The size of the memory is the size of the Mealy machine.

For example, a finite memory strategy for the game of \figref{tj:fig:infmemoryneeded} that loops 42 times in $q_0$ then goes to $q_1$, loops once there, and goes back to $q_0$ to start again can be represented by the machine of \figref{tj:fig:memory42}.
This machine has 46 states, since it needs to count how many times it has accumulated energy in $q_0$.
Note that if starting in $q_1$, the machine first goes back to $q_0$ then applies the aforementioned strategy.
Also, a strategy must be complete on its input, hence it must allow $q_1$ to occur in any memory state (in this case it goes back to the initial memory state and to $q_0$ in the game).

\begin{figure}
\centering
\begin{tikzpicture}[auto, thick,>=stealth]
\useasboundingbox (-1.25,1.45) rectangle (7.8,-3.15);
\node[state, initial,initial text=] (m0) at (0,0) {$m_0$};
\node[state] (m1) at (2,0) {$m_1$};
\node (dots) at (3.5,0) {\dots};
\node[state] (m42) at (5,0) {$m_{42}$};
\node[state] (m43) at (7,0) {$m_{43}$};
\node[state] (m44) at (7,-2) {$m_{44}$};
\node[state] (m45) at (5,-2) {$m_{45}$};

\draw[->] (m0) edge[loop above] node {$q_1 | q_0$} (m0);
\draw[->] (m0) edge node {$q_0 | q_0$} (m1);
\draw[->] (m1) edge node {$q_0 | q_0$} (dots);
\draw[->] (m42) edge node {$q_0 | q_0$} (m43);
\draw[->] (m43) edge node {$q_0 | q_1$} (m44);
\draw[->] (m44) edge node {$q_1 | q_1$} (m45);
\draw[->] (m45) edge[bend left] node[swap,pos=0.15] {$q_1 | q_0$} (m0);

\draw[->] (m1) edge[bend left=15] node[pos=0.15] {$q_1 | q_0$} (m0);
\draw[->] (m42) edge[bend left=25] node[,pos=0.15] {$q_1 | q_0$} (m0);
\draw[->] (m43) edge[bend left=45] node[swap,pos=0.33] {$q_1 | q_0$} (m0);
\draw[->] (m44) edge[bend left=60, in =60] node[pos=0.75] {$q_0 | q_0$} (m0);
\draw[->] (m45) edge[bend left=60] node[pos=0.1] {$q_0 | q_0$} (m0);
\end{tikzpicture}
\caption[A Mealy machine representing a finite memory strategy.]{A Mealy machine representing a finite memory strategy (46 states). The input of edges is the current state of the game, the output is the chosen next state.}
\label{tj:fig:memory42}
\end{figure}

\begin{remark}
Note that the strategies with given memory may not be optimal.
Consider for example the game of \figref{tj:fig:infmemoryneeded} with objective being to maintain the energy level above $0$ and ensure a mean-payoff greater than or equal to $\frac1{43}$.

An infinite memory strategy can win this game.
It is defined by phases, as follows.
At the $k$-th phase, loop $42k$ times in $q_0$, then go to $q_1$, loop $k$ times, go back to $q_0$.
This ensures that the transitions between $q_0$ and $q_1$ are negligible, hence the limit of the payoff is $\frac1{43}$, since 43 steps are needed to increment the payoff counter once.
In addition, at each phase the energy goes back to its initial value (which can be $1$), while only encountering positive values.

On the other hand, if only $k$ states of memory are allowed, the best payoff achievable is $\frac{k}{43k+2}$ (the corresponding strategy consists in repeating phase $k$).
Hence not only the optimal payoff cannot be achieved, but allowing more memory allows to achieve better payoff.
\end{remark}

Solving the game assuming player~0 has bounded memory $k$ given as input consists in guessing this strategy as a machine, which is an exponential object if $k$ is given in binary.
Then the machine is synchronized with the game, yielding a single-player game, to be played by player~1.
In this game, only memoryless strategies need to be considered.

Indeed, any infinite path not satisfying $p_e \geq c_e \wedge p_v \geq c_v$ either is such that $p_e$ falls below $c_e$ or the limit average of $p_v$ is below $c_v$.
In both cases this amounts to finding a lasso path in the graph, which can be guessed.

Regarding complexity, the above procedure is in $\NEXPSPACE$ (which is equivalent to $\EXPSPACE$~\cite[Chap.~20]{papadimitriou94}), although the bound is not tight.
